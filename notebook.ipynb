{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in Neural Networks using Noise Contrastive Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Programming 6998 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This section is based on **arXiv:1807.09289**: Hafner, Tran, Lillicrap: \"Reliable Uncertainty Estimates\", https://arxiv.org/abs/1807.09289)\n",
    "\n",
    "Neural networks are often very successful at making predictions for inputs that are in some sense similar to the training data. However, if the training data is not sufficiently diverse, then at test time, one will often encounter inputs that are *out-of-distribution (OOD)* and for which the network might yield unpredictable and inaccurate results -- as opposed to the *in-distribution (ID)* training data. In those cases, it would therefore be useful to have reliable estimates on the uncertainty of the prediction.\n",
    "\n",
    "Bayesian neural networks are a standard way of tackling this problem. During training, instead of learning point estimates for the weights and biases of the network, one learns a probability distribution over those parameters. At test time, one first samples the network parameters from the learned distributions before making a prediction. As such, a Bayesian neural network represents a distribution of functions, which for a given input yields a certain distribution of outputs. \n",
    "However, it is not clear exactly how to specify the prior distribution on the weights, or how such a network generalizes on OOD data seems rather arbitrary.\n",
    "\n",
    "A simple toy example is given in the following figure. A neural network is used to predict the mean and standard deviation of a scalar variable (it has a two-dimensional output layer).\n",
    "On the left, a simple deterministic network is used. On the right, a bayesian layer is introduced just before the output layer.\n",
    "\n",
    "\n",
    "<img src=\"./images/nn.png\" width=\"600\" /> <img src=\"./images/nn1.png\" width=\"300\" />\n",
    "\n",
    "Even though the Bayesian approach does introduce uncertainty in the predicted mean (which depends on the posterior of the weights in the final hidden layer), the generalization to unseen data points is, in some sense, random.\n",
    "\n",
    "A recently proposed approach starts from the premise that priors in the data space are better behaved than the usual neural network priors in weight space, and in order to encourage the network to output high uncertainty it is enough to encourage this at the boundary of the training data. The procedure is as follows:\n",
    "1. Perturb the input data to approximate OOD behavior (e.g. add noise) \n",
    "2. Stimulate the network to output a high uncertainty on the OOD data, by adding an additional contribution to the loss function.\n",
    "\n",
    "\n",
    "In the example of the Bayesian neural network, the proposed new loss function then looks like this\n",
    "\n",
    "$$\\large \\mathcal{L}_{\\text{NCP}} (\\phi) = \\mathcal{L}_{\\text{BBB}}(\\phi) {\\Big\\rvert}_{\\text{ID}} \\quad + \\quad \\lambda \\text{KL}\\left[ \\text{Normal}(\\mu_{\\mu}, \\sigma_{\\mu}^2) || q(\\mu(x)) \\right]\\Big\\rvert_{\\text{OOD}}$$\n",
    "\n",
    "in which the variance of the normal distribution $\\sigma^2$ is chosen very large, to stimulate uncertainty in the distribution of the mean when the network is fed with OOD inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: MNIST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our project attempts to apply similar ideas in a classification setting. We want to see whether it is possible to train a neural network to output a higher uncertainty on unseen data and prevent overconfident classification, by adding an additional contribution to the loss function, similar to the previously discussed paper. The following graph explains the setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram.png\" width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification setting, we cannot just use the output distribution as a measure for uncertainty. **However, we can use the entropy of the probabilities from the softmax output layer to represent the uncertainty of the classifier.** This is an easy-to-calculate quantity. \n",
    "\n",
    "As for the generation of OOD data, multiple possibilities exist. Here, we chose to **apply affine transformations to the images.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Generate OOD Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is generally “in-distribution” (similar to other data we have observed) whereas sometimes the test data can be drawn from a different distribution, i.e. it is “out-of-distribution” (OOD). We don’t want our uncertainty estimates to be overconfident on this OOD data which is inherently different from the in-distribution data. If we actually had an analytical form or a way of sampling the out-of-distribution data, we could just use that in training, but in general we don’t have this. Instead, we can attempt to generate OOD data to train on (in addition to our regular “in-distribution” training data), and encourage the model to output high uncertainty for this synthetically generated OOD data. This leads us to a procedure for generating OOD data.\n",
    "\n",
    "For our **MNIST classification testbed**, we can make a very intuitive in vs. out of distribution split as follows: we take K of the digit classes as in-distribution, and the remaining 10 - K classes as the out-of-distribution data. Clearly, if the model is trained on e.g. digits {0,1,2,3,4,5,6,7} but never sees {8,9}, then we expect: \n",
    "\n",
    "1) it will perform “well” on the in-distribution data and \"poorly\" on the omitted OOD data, and \n",
    "\n",
    "2) have greater certainty in classification decisions on in-distribution digit classes than on the omitted OOD digit classes. \n",
    "\n",
    "We thus have a legitimate way of making OOD data. Meanwhile, during training, we want to generate something that looks like this OOD data. As a simple proof of concept to generate some type of OOD data, we take a given image and apply a transformation to it. This perturbs the image so that it may move away from the data manifold and into an out-of-distribution region. However, this is not always the case: some perturbations may only slightly change the image so that it is still “in-distribution.” This procedure is ill-posed because what does it mean to generate the complement of the in-distribution training set for a complicated dataset? I.e. what does it look like to be **not** a \"7\" or \"4\" or a dog?\n",
    "\n",
    "For our demonstration, we rotate the image by a random amount. This is just one of many possible transformations we could imagine applying. For example, we could generalize rotation to any kind of random affine transformation with arbitrary translation, rotation, scaling, and shear. Even more generally, we could apply a projective transformation. Furthermore, for MNIST we could do arbitrary pixelwise transformations on the grayscale intensities (or for different color channels if there is more than one), or add noise (e.g. Gaussian jitter on each pixel). Also, complex warpings and other transformations are possible, e.g. swirl, or any arbitrary deformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for MNIST (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Deterministic network -- **no Bayesian layers!**\n",
    "\n",
    "2. Input data are 28x28 images of the digits 1, 2, 3, 4, 5, 6, 7 -- **8 and 9 are omitted, and are used for evaluation.**\n",
    "\n",
    "3. 256 --> 256 --> 8 network, using leaky ReLU activation functions and a softmax output.\n",
    "\n",
    "4. Training through Adam.\n",
    "\n",
    "5. **OOD data generated through rotations**\n",
    "\n",
    "<img src=\"./images/layouta.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "## download MNIST data\n",
    "from ncp_classifier.datasets.mnist import init\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network layout\n",
    "def network(data, layer_sizes, ncp_scale = 0.1):\n",
    "    '''\n",
    "    Defines network topology \n",
    "    '''\n",
    "    # Define neural network topology (in this case, a simple MLP)\n",
    "    hidden = data[0]\n",
    "    labels = data[1]\n",
    "    for size in layer_sizes[:-1]:\n",
    "        hidden = tf.layers.dense(\n",
    "                inputs=hidden,\n",
    "                units=size,\n",
    "                activation=tf.nn.leaky_relu\n",
    "                )\n",
    "    logits = tf.layers.dense(inputs=hidden, units=layer_sizes[-1], activation=None)\n",
    "    # computes the traditional cross-entropy loss, \n",
    "    # which we want to minimize over the in-distribution training data\n",
    "    standard_loss=tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "            )\n",
    "    # computes the ncp_loss, in this case simply the entropy, which we want to minimize over the out-of-distribution training data\n",
    "    logits = logits - tf.reduce_mean(logits)\n",
    "    class_probabilities = tf.nn.softmax(logits * tf.constant(ncp_scale, dtype=tf.float32))\n",
    "    entropy = -class_probabilities * tf.log(tf.clip_by_value(class_probabilities, 1e-20, 1))\n",
    "    # Use the normalized entropy (divide by log_b(K) ) so is on [0,1] \n",
    "    # so easier to compare across experiments:\n",
    "    if NORMALIZE_ENTROPY == True:\n",
    "        baseK = tf.constant(layer_sizes[-1], dtype=tf.float32, shape=(layer_sizes[-1], ))\n",
    "        entropy /= tf.log(baseK)\n",
    "    mean, variance = tf.nn.moments(entropy, axes=[1])\n",
    "    ncp_loss = tf.reduce_mean(mean)\n",
    "    ncp_std = tf.reduce_mean(tf.math.sqrt(variance))\n",
    "    return standard_loss, ncp_loss, logits, class_probabilities, ncp_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network_tpl = tf.make_template(\n",
    "#    'network',\n",
    "#    network,\n",
    "#    layer_sizes=logging['layer_sizes'],\n",
    "#    ncp_scale=logging['ncp_scale']\n",
    "#    )\n",
    "# id_loss, id_ncp_loss, id_logits, _, id_ncp_std  = network_tpl(id_data)  # calculate CE loss for id input data\n",
    "# od_loss, od_ncp_loss, od_logits, _, od_ncp_std = network_tpl(od_data)  # calculate entropy for od input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss = alpha * id_loss - (1 - alpha) * od_ncp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for deterministic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform several experiments to demonstrate how uncertainty can be estimated to prevent overconfident classification and how NCP’s can aid in this process. For a K-category classification task, one reasonable measure of uncertainty is the entropy over the length K softmax output vector. (We also discuss alternatives in the Future Work section). One experiment is described below, and two others are provided in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: “alpha”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to see how the network’s uncertainty varies as a function of alpha, the weighting parameter in the loss function which trades off the standard cross-entropy loss for classification against the uncertainty term. We look at the properties of three partitions of the dataset:\n",
    "\n",
    "•\t“in-distribution” or “id”: the original training set MNIST digits\n",
    "\n",
    "•\t“out-of-distribution” or “OOD” or “od”: those images that were transformed to represent OOD data\n",
    "\n",
    "•\t“omitted” or “om”: the holdout digit classes completely omitted from training\n",
    "\n",
    "Once the network is fully trained, for each of these partitions, evaluate the mean uncertainty over that entire partition of the data. E.g. for the “out-of-distribution” data, we can check the mean uncertainty over all OOD instances. We also look at basic statistics like the standard deviation of this quantity. \n",
    "Also, we want to make sure that the classification accuracy remains good while still giving reasonable uncertainty estimates.\n",
    "\n",
    "Explanation:\n",
    "We observe a general trend where accuracy is traded off for larger uncertainty estimates (alpha is related to the inverse of the uncertainty weighting, so as alpha goes to 0, the uncertainty loss is given higher and higher weight). But over a large range of values, the network can achieve good classification accuracy and still output large uncertainties for OOD data.\n",
    "\n",
    "Note: the fact that the OOD accuracy is still reasonably high brings into question how good a job this transformation does at moving the data away from the data manifold and into an OOD region: because accuracy is still relatively high, perhaps it does not move the point very far?\n",
    "Also, note that the omitted data (\"om\") classification accuracy is 0 always because the network is not evencapable of predicting those categories (although it would be interesting to include an “other” or “anomaly” category as a catch-all category for anything too new, as discussed later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1e-07\n",
      "Epoch: 0001 cost=24.975974242\n",
      "Epoch: 0002 cost=13.169512903\n",
      "Epoch: 0003 cost=9.372821758\n",
      "Epoch: 0004 cost=7.024959794\n",
      "Epoch: 0005 cost=5.371150734\n",
      "Epoch: 0006 cost=4.189217833\n",
      "Epoch: 0007 cost=3.361607764\n",
      "Epoch: 0008 cost=2.779235546\n",
      "Epoch: 0009 cost=2.392999147\n",
      "Epoch: 0010 cost=2.152677618\n",
      "Epoch: 0011 cost=1.991151148\n",
      "Epoch: 0012 cost=1.874213417\n",
      "Epoch: 0013 cost=1.785440108\n",
      "Epoch: 0014 cost=1.720613751\n",
      "Epoch: 0015 cost=1.674968458\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.37866193\n",
      "Full od_acc: 0.20349859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_det.py:193: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__alpha_0.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 5.62341325190349e-07\n",
      "Epoch: 0001 cost=22.636331688\n",
      "Epoch: 0002 cost=10.152962472\n",
      "Epoch: 0003 cost=6.458519733\n",
      "Epoch: 0004 cost=4.422609669\n",
      "Epoch: 0005 cost=3.147070657\n",
      "Epoch: 0006 cost=2.347727595\n",
      "Epoch: 0007 cost=1.832818960\n",
      "Epoch: 0008 cost=1.501410993\n",
      "Epoch: 0009 cost=1.276239922\n",
      "Epoch: 0010 cost=1.140459567\n",
      "Epoch: 0011 cost=1.048024585\n",
      "Epoch: 0012 cost=0.982088020\n",
      "Epoch: 0013 cost=0.926946611\n",
      "Epoch: 0014 cost=0.882430612\n",
      "Epoch: 0015 cost=0.840814948\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.79092807\n",
      "Full od_acc: 0.34016347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_det.py:193: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__alpha_1.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 3.162277660168379e-06\n",
      "Epoch: 0001 cost=15.960980630\n",
      "Epoch: 0002 cost=4.885454197\n",
      "Epoch: 0003 cost=2.767755221\n",
      "Epoch: 0004 cost=1.759029398\n",
      "Epoch: 0005 cost=1.206281020\n",
      "Epoch: 0006 cost=0.889243320\n",
      "Epoch: 0007 cost=0.690560057\n",
      "Epoch: 0008 cost=0.579700486\n",
      "Epoch: 0009 cost=0.510158610\n",
      "Epoch: 0010 cost=0.458015509\n",
      "Epoch: 0011 cost=0.419436774\n",
      "Epoch: 0012 cost=0.387386701\n",
      "Epoch: 0013 cost=0.358688111\n",
      "Epoch: 0014 cost=0.332650278\n",
      "Epoch: 0015 cost=0.308003272\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9129101\n",
      "Full od_acc: 0.3214294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_det.py:193: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__alpha_2.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.778279410038923e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b623becaa905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mncp_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist_det\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0malpha_experiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0malpha_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m## During training, log files will be periodically saved in ncp_classifier/logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_det.py\u001b[0m in \u001b[0;36malpha_experiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[0mood_transformations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[0mALPHA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 experiment_suffix)\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_det.py\u001b[0m in \u001b[0;36mrun_single\u001b[1;34m(digits_to_omit, ood_transformations, alpha, experiment_suffix)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mid_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mom_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mom_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_partial_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdigits_to_omit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# Generate out-of-distribution images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mod_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mod_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_od_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mood_transformations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0mid_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mod_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mod_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mod_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_utils.py\u001b[0m in \u001b[0;36mgenerate_od_data\u001b[1;34m(images, labels, transformations, plot)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mangles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 od_images = np.concatenate(\n\u001b[1;32m---> 60\u001b[1;33m                         \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                         axis=0)\n\u001b[0;32m     62\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'translate'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mangles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 od_images = np.concatenate(\n\u001b[1;32m---> 60\u001b[1;33m                         \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                         axis=0)\n\u001b[0;32m     62\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'translate'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(image, angle, resize, center, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     return warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m--> 385\u001b[1;33m                 mode=mode, cval=cval, clip=clip, preserve_range=preserve_range)\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mwarp\u001b[1;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[0;32m    842\u001b[0m                 warped = _warp_fast(image, matrix,\n\u001b[0;32m    843\u001b[0m                                     \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m                                     order=order, mode=mode, cval=cval)\n\u001b[0m\u001b[0;32m    845\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m                 \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mskimage\\transform\\_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m     \"\"\"Convert the input to an array.\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## code running deterministic network, on the alpha experiment\n",
    "## parameters and more details are in the file itself:\n",
    "## see ncp_classifier/models/mnist_det.py\n",
    "\n",
    "#Reset tf graph in case you have already run one of our other experiments in this notebook session:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from ncp_classifier.models.mnist_det import alpha_experiment\n",
    "\n",
    "alpha_experiment()\n",
    "\n",
    "## During training, log files will be periodically saved in ncp_classifier/logs\n",
    "#Full id_acc:     accuracy over the entire partition of in-distribution data\n",
    "#Full od_acc:     accuracy over the entire partition of out-of-distribution data\n",
    "    \n",
    "#(Ignore Futurewarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the plotting script\n",
    "run_plotting('alpha')\n",
    "\n",
    "#Output is saved in ncp_classifier/output/alpha\n",
    "#Example plots are shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/det_alpha_entropy.png\" width=\"500\" />\n",
    "\n",
    "<img src=\"./images/det_alpha_accuracy.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY OBSERVATIONS\n",
    "\n",
    "1. Accuracy decreases as NCP term in loss is weighted more strongly (toward left side of figure above)\n",
    "\n",
    "2. Entropy difference between ID and OOD data increases, but the variances are relatively high --> **network still unable to tell the difference between input it has already seen vs. new input**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for MNIST (2): addition of bayesian layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./images/layoutb.png\" width=\"500\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(data, layer_sizes=[256, 256], ncp_scale=0.1):\n",
    "    '''\n",
    "    Defines network topology\n",
    "    '''\n",
    "    hidden = data[0]\n",
    "    labels = data[1]\n",
    "    for size in layer_sizes[:-1]:\n",
    "        hidden = tf.layers.dense(\n",
    "                inputs=hidden,\n",
    "                units=size,\n",
    "                activation=tf.nn.leaky_relu\n",
    "                )\n",
    "    weight_std = 0.1\n",
    "    init_std = np.log(np.exp(weight_std) - 1).astype(np.float32)\n",
    "    kernel_posterior = tfd.Independent(tfd.Normal(\n",
    "        tf.get_variable(\n",
    "            'kernel_mean',\n",
    "            (hidden.shape[-1].value, layer_sizes[-1]),\n",
    "            tf.float32,\n",
    "            tf.random_normal_initializer(0, weight_std)),\n",
    "        tf.nn.softplus(tf.get_variable(\n",
    "            'kernel_std',\n",
    "            (hidden.shape[-1].value, layer_sizes[-1]),\n",
    "            tf.float32,\n",
    "            tf.constant_initializer(init_std)))), 2)\n",
    "    kernel_prior = tfd.Independent(tfd.Normal(\n",
    "        tf.zeros_like(kernel_posterior.mean()),\n",
    "        tf.zeros_like(kernel_posterior.mean()) + tf.nn.softplus(init_std)), 2)\n",
    "\n",
    "    bias_prior = None\n",
    "    bias_posterior = tfd.Deterministic(tf.get_variable(\n",
    "        'bias_mean',\n",
    "        (layer_sizes[-1],),\n",
    "        tf.float32,\n",
    "        tf.constant_initializer(0.0)))\n",
    "    logits = tfp.layers.DenseReparameterization(\n",
    "        layer_sizes[-1],\n",
    "        kernel_prior_fn=lambda *args, **kwargs: kernel_prior,\n",
    "        kernel_posterior_fn=lambda *args, **kwargs: kernel_posterior,\n",
    "        bias_prior_fn=lambda *args, **kwargs: bias_prior,\n",
    "        bias_posterior_fn=lambda *args, **kwargs: bias_posterior)(hidden)\n",
    "\n",
    "    standard_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits,\n",
    "                labels=labels)\n",
    "            )\n",
    "    logits = logits - tf.reduce_mean(logits)\n",
    "    class_probabilities = tf.nn.softmax(\n",
    "            logits * tf.constant(ncp_scale, dtype=tf.float32))\n",
    "    entropy = -class_probabilities * tf.log(\n",
    "            tf.clip_by_value(class_probabilities, 1e-20, 1))\n",
    "    if NORMALIZE_ENTROPY is True:\n",
    "        baseK = tf.constant(\n",
    "                layer_sizes[-1],\n",
    "                dtype=tf.float32,\n",
    "                shape=(layer_sizes[-1],))\n",
    "        entropy /= tf.log(baseK)\n",
    "    mean, variance = tf.nn.moments(entropy, axes=[1])\n",
    "    ncp_loss = tf.reduce_mean(mean)\n",
    "    ncp_std = tf.reduce_mean(tf.math.sqrt(variance))\n",
    "    return standard_loss, ncp_loss, logits, ncp_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Anaconda3\\lib\\site-packages\\skimage\\__init__.py:71: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\Grey\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytest.py' mode='r' encoding='utf-8'>\n",
      "  imp.find_module('pytest')\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Grey\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost=1.385920509\n",
      "Epoch: 0002 cost=0.260288010\n",
      "Epoch: 0003 cost=0.155769217\n",
      "Epoch: 0004 cost=0.127837482\n",
      "Epoch: 0005 cost=0.114170436\n",
      "Epoch: 0006 cost=0.113825693\n",
      "Epoch: 0007 cost=0.101082829\n",
      "Epoch: 0008 cost=0.112927634\n",
      "Epoch: 0009 cost=0.107083318\n",
      "Epoch: 0010 cost=0.110372019\n",
      "Epoch: 0011 cost=0.110643857\n",
      "Epoch: 0012 cost=0.096120410\n",
      "Epoch: 0013 cost=0.077545325\n",
      "Epoch: 0014 cost=0.077223694\n",
      "Epoch: 0015 cost=0.116040999\n",
      "Epoch: 0016 cost=0.061386717\n",
      "Epoch: 0017 cost=0.063121414\n",
      "Epoch: 0018 cost=0.064915277\n",
      "Epoch: 0019 cost=0.064451125\n",
      "Epoch: 0020 cost=0.062958490\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9887759\n",
      "Full od_acc: 0.28844398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_0.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.401556650\n",
      "Epoch: 0002 cost=0.261669069\n",
      "Epoch: 0003 cost=0.165492805\n",
      "Epoch: 0004 cost=0.128131353\n",
      "Epoch: 0005 cost=0.112207308\n",
      "Epoch: 0006 cost=0.116970898\n",
      "Epoch: 0007 cost=0.105518784\n",
      "Epoch: 0008 cost=0.107642690\n",
      "Epoch: 0009 cost=0.118183563\n",
      "Epoch: 0010 cost=0.132875464\n",
      "Epoch: 0011 cost=0.091409684\n",
      "Epoch: 0012 cost=0.096308317\n",
      "Epoch: 0013 cost=0.083674399\n",
      "Epoch: 0014 cost=0.084087519\n",
      "Epoch: 0015 cost=0.083728114\n",
      "Epoch: 0016 cost=0.083450293\n",
      "Epoch: 0017 cost=0.068298592\n",
      "Epoch: 0018 cost=0.084664288\n",
      "Epoch: 0019 cost=0.066607926\n",
      "Epoch: 0020 cost=0.055899209\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9842946\n",
      "Full od_acc: 0.2263278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_1.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.415993463\n",
      "Epoch: 0002 cost=0.269202844\n",
      "Epoch: 0003 cost=0.166176918\n",
      "Epoch: 0004 cost=0.128974009\n",
      "Epoch: 0005 cost=0.115458864\n",
      "Epoch: 0006 cost=0.114192300\n",
      "Epoch: 0007 cost=0.129313072\n",
      "Epoch: 0008 cost=0.105124109\n",
      "Epoch: 0009 cost=0.098642305\n",
      "Epoch: 0010 cost=0.111410574\n",
      "Epoch: 0011 cost=0.113569829\n",
      "Epoch: 0012 cost=0.125822809\n",
      "Epoch: 0013 cost=0.090212189\n",
      "Epoch: 0014 cost=0.079191717\n",
      "Epoch: 0015 cost=0.081612732\n",
      "Epoch: 0016 cost=0.085531130\n",
      "Epoch: 0017 cost=0.082816751\n",
      "Epoch: 0018 cost=0.062391651\n",
      "Epoch: 0019 cost=0.076741228\n",
      "Epoch: 0020 cost=0.075102939\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.984917\n",
      "Full od_acc: 0.1742946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_2.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.414175890\n",
      "Epoch: 0002 cost=0.266679985\n",
      "Epoch: 0003 cost=0.173073583\n",
      "Epoch: 0004 cost=0.138220014\n",
      "Epoch: 0005 cost=0.126533599\n",
      "Epoch: 0006 cost=0.104097093\n",
      "Epoch: 0007 cost=0.127592015\n",
      "Epoch: 0008 cost=0.118197800\n",
      "Epoch: 0009 cost=0.102483354\n",
      "Epoch: 0010 cost=0.110217021\n",
      "Epoch: 0011 cost=0.125598973\n",
      "Epoch: 0012 cost=0.090255968\n",
      "Epoch: 0013 cost=0.081311449\n",
      "Epoch: 0014 cost=0.099984785\n",
      "Epoch: 0015 cost=0.090556699\n",
      "Epoch: 0016 cost=0.069995164\n",
      "Epoch: 0017 cost=0.061335341\n",
      "Epoch: 0018 cost=0.082666107\n",
      "Epoch: 0019 cost=0.075507766\n",
      "Epoch: 0020 cost=0.084146202\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9843361\n",
      "Full od_acc: 0.37981328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_3.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.430252694\n",
      "Epoch: 0002 cost=0.274533997\n",
      "Epoch: 0003 cost=0.174690929\n",
      "Epoch: 0004 cost=0.138522227\n",
      "Epoch: 0005 cost=0.114805660\n",
      "Epoch: 0006 cost=0.107113865\n",
      "Epoch: 0007 cost=0.110320976\n",
      "Epoch: 0008 cost=0.123253561\n",
      "Epoch: 0009 cost=0.122440584\n",
      "Epoch: 0010 cost=0.112358964\n",
      "Epoch: 0011 cost=0.104228913\n",
      "Epoch: 0012 cost=0.098558531\n",
      "Epoch: 0013 cost=0.086626781\n",
      "Epoch: 0014 cost=0.076850010\n",
      "Epoch: 0015 cost=0.101666160\n",
      "Epoch: 0016 cost=0.093148136\n",
      "Epoch: 0017 cost=0.087364781\n",
      "Epoch: 0018 cost=0.057816398\n",
      "Epoch: 0019 cost=0.057671977\n",
      "Epoch: 0020 cost=0.060922634\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9896473\n",
      "Full od_acc: 0.26259336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_4.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.456774112\n",
      "Epoch: 0002 cost=0.263717954\n",
      "Epoch: 0003 cost=0.171987178\n",
      "Epoch: 0004 cost=0.132333702\n",
      "Epoch: 0005 cost=0.116206844\n",
      "Epoch: 0006 cost=0.116332592\n",
      "Epoch: 0007 cost=0.116534928\n",
      "Epoch: 0008 cost=0.115032876\n",
      "Epoch: 0009 cost=0.108660011\n",
      "Epoch: 0010 cost=0.103829893\n",
      "Epoch: 0011 cost=0.125688366\n",
      "Epoch: 0012 cost=0.104688706\n",
      "Epoch: 0013 cost=0.071455682\n",
      "Epoch: 0014 cost=0.084175305\n",
      "Epoch: 0015 cost=0.109452106\n",
      "Epoch: 0016 cost=0.072623637\n",
      "Epoch: 0017 cost=0.082576785\n",
      "Epoch: 0018 cost=0.086227874\n",
      "Epoch: 0019 cost=0.052669654\n",
      "Epoch: 0020 cost=0.050251905\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.98470956\n",
      "Full od_acc: 0.18678424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_5.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.445305963\n",
      "Epoch: 0002 cost=0.262265727\n",
      "Epoch: 0003 cost=0.174942650\n",
      "Epoch: 0004 cost=0.131063823\n",
      "Epoch: 0005 cost=0.119551799\n",
      "Epoch: 0006 cost=0.102295861\n",
      "Epoch: 0007 cost=0.131355289\n",
      "Epoch: 0008 cost=0.122235612\n",
      "Epoch: 0009 cost=0.102608946\n",
      "Epoch: 0010 cost=0.102187467\n",
      "Epoch: 0011 cost=0.102250096\n",
      "Epoch: 0012 cost=0.098785785\n",
      "Epoch: 0013 cost=0.114411853\n",
      "Epoch: 0014 cost=0.077399770\n",
      "Epoch: 0015 cost=0.071134232\n",
      "Epoch: 0016 cost=0.085112779\n",
      "Epoch: 0017 cost=0.086233776\n",
      "Epoch: 0018 cost=0.071606815\n",
      "Epoch: 0019 cost=0.060250390\n",
      "Epoch: 0020 cost=0.068694641\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9880913\n",
      "Full od_acc: 0.18317427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_6.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.436690042\n",
      "Epoch: 0002 cost=0.263301497\n",
      "Epoch: 0003 cost=0.169596112\n",
      "Epoch: 0004 cost=0.135675165\n",
      "Epoch: 0005 cost=0.114188269\n",
      "Epoch: 0006 cost=0.123624373\n",
      "Epoch: 0007 cost=0.122770524\n",
      "Epoch: 0008 cost=0.107927109\n",
      "Epoch: 0009 cost=0.124109505\n",
      "Epoch: 0010 cost=0.111593101\n",
      "Epoch: 0011 cost=0.116172210\n",
      "Epoch: 0012 cost=0.086585968\n",
      "Epoch: 0013 cost=0.115434562\n",
      "Epoch: 0014 cost=0.073888725\n",
      "Epoch: 0015 cost=0.070838030\n",
      "Epoch: 0016 cost=0.091828810\n",
      "Epoch: 0017 cost=0.075095684\n",
      "Epoch: 0018 cost=0.080244831\n",
      "Epoch: 0019 cost=0.065618461\n",
      "Epoch: 0020 cost=0.096415291\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9841909\n",
      "Full od_acc: 0.28838176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_7.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.431007665\n",
      "Epoch: 0002 cost=0.265647287\n",
      "Epoch: 0003 cost=0.170975210\n",
      "Epoch: 0004 cost=0.132090695\n",
      "Epoch: 0005 cost=0.120762899\n",
      "Epoch: 0006 cost=0.106702059\n",
      "Epoch: 0007 cost=0.118220635\n",
      "Epoch: 0008 cost=0.127847743\n",
      "Epoch: 0009 cost=0.112342823\n",
      "Epoch: 0010 cost=0.102962083\n",
      "Epoch: 0011 cost=0.100329899\n",
      "Epoch: 0012 cost=0.085683854\n",
      "Epoch: 0013 cost=0.095772201\n",
      "Epoch: 0014 cost=0.094640945\n",
      "Epoch: 0015 cost=0.085296139\n",
      "Epoch: 0016 cost=0.087824001\n",
      "Epoch: 0017 cost=0.066217948\n",
      "Epoch: 0018 cost=0.088257882\n",
      "Epoch: 0019 cost=0.060709250\n",
      "Epoch: 0020 cost=0.057652198\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9784647\n",
      "Full od_acc: 0.19904564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_8.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=1.429397490\n",
      "Epoch: 0002 cost=0.263364158\n",
      "Epoch: 0003 cost=0.167762156\n",
      "Epoch: 0004 cost=0.133689059\n",
      "Epoch: 0005 cost=0.113967265\n",
      "Epoch: 0006 cost=0.116958645\n",
      "Epoch: 0007 cost=0.118398267\n",
      "Epoch: 0008 cost=0.122569570\n",
      "Epoch: 0009 cost=0.117463648\n",
      "Epoch: 0010 cost=0.092174112\n",
      "Epoch: 0011 cost=0.103138497\n",
      "Epoch: 0012 cost=0.124186643\n",
      "Epoch: 0013 cost=0.086301818\n",
      "Epoch: 0014 cost=0.075712571\n",
      "Epoch: 0015 cost=0.090721949\n",
      "Epoch: 0016 cost=0.091949441\n",
      "Epoch: 0017 cost=0.062874140\n",
      "Epoch: 0018 cost=0.070409531\n",
      "Epoch: 0019 cost=0.061815710\n",
      "Epoch: 0020 cost=0.077599688\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9781535\n",
      "Full od_acc: 0.21954356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey\\Desktop\\2018Fall\\ProbabilisticProgramming\\FinalProject\\ProbPro\\ProbabilisticProgramming6998\\ncp_classifier\\models\\mnist_bayesian.py:249: ResourceWarning: unclosed file <_io.BufferedWriter name='ncp_classifier\\\\logs\\\\log_ncp_on__rotate_9.p'>\n",
      "  pickle.dump(logging, open(logpath, \"wb\" ) )\n"
     ]
    }
   ],
   "source": [
    "# code running bayesian network, on the rotate experiment\n",
    "# parameters and mroe details are in the file itself:\n",
    "# see ncp_classifier/models/mnist_bayesian.py\n",
    "\n",
    "#Reset tf graph in case you have already run one of our other experiments in this notebook session:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from ncp_classifier.models.mnist_bayesian import rotation_experiment\n",
    "\n",
    "rotation_experiment()\n",
    "\n",
    "## During training, log files will be periodically saved in ncp_classifier/logs\n",
    "#Full id_acc:     accuracy over the entire partition of in-distribution data\n",
    "#Full od_acc:     accuracy over the entire partition of out-of-distribution data\n",
    "    \n",
    "#(Ignore Futurewarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for Bayesian network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./images/bayesian1.png\" width=\"500\" />\n",
    "\n",
    "<img src=\"./images/bayesian2.png\" width=\"500\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ongoing / Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We used a simple few layer MLP which is good enough for MNIST but more powerful NN architectures can still benefit from this Bayesian perspective and from NCPs.\n",
    "\n",
    "-It is worth trying other experiments to generate OOD instances and gain some understanding of how the in-distribution regions transition into the out-of-distribution regions. In particular, two kinds of interpolations could be interesting. One is to **interpolate between in-distribution data to out-of-distribution data**: borrow ideas from the \"Mixup\" training procedure, but apply them at test time as a form of analysis instead of during training as a form of data augmentation. In Mixup, you would take a convex combination in data space between pairs of inputs [and same for labels]. But we could do this: for a holdout set, we use ideas of Mixup and vary a parameter lambda from [0,1] so the inputs go from being in-distribution to being OOD. So at test time, evaluate a series of inputs which are made by interpolating between an in-distribution training point to something that is definitely OOD (an MNIST omitted holdout digit). Then look at the uncertainty as a function of lambda [i.e. uncertainty as a function of distance away from in-distribution]. Roughly speaking, in a Bayesian NN with NCPs we would expect uncertainty to increase as we go from in-distribution to OOD, vs. without NCPs maybe we remain overconfident and the uncertainty may not change much as we go to something that we know is actually OOD. But to confirm this could be interesting. One concern though is that linearly interpolating between e.g. a \"7\" and a \"9\", directly in data space, may not be as easily interpretable as we would hope, e.g. it could have intermediate interpolations which are neither \"7\" nor \"9\" like, so maybe the OOD \"9\" with lambda=1 would actually look more like a realistic input than any of the intermediate interpolations, so in other words, there would be no reason to think that this graph of uncertainty would be monotonically increasing as we get further from the in-distribution space. But it might still be interesting to see and compare to a regular Bayesian NN without NCPs.\n",
    "\n",
    "-On the other hand, we can also **interpolate between two categories of in-distribution data**. For the MNIST digits, this could be e.g. going from a “3” to a “7.” We would expect that for data as high dimensional as MNIST images, the set would be non-convex. So when we interpolate between two points in the set, we are very likely to move outside of the set and get to OOD regions. The issue here is how meaningful those regions may be. Although they would be OOD, would they be useful/realistic? What does a digit that is halfway between a “3” and a “7” tell you about digits you have never seen before?\n",
    "\n",
    "\n",
    "-Another very cool experiment could build on the KL divergence term of the NCP loss used by Hafner et al. In the univariate regression setting, they use a network with two output nodes; one for a gaussian mean and the other for it's variance. Extending their ideas for regression to the classification setting is very useful but would require some changes. This would be a new novelty since not only are NCP’s fairly new (1st version of paper submitted in July 2018) but it also seems like the authors focused on active learning in the regression setting so extending this to classification is a new area. Here we outline one possibility for this approach. The KL divergence NCP term requires a prior on the label space. If we think of the network's final output K-category softmax vector as belonging to a K-dimensional distribution, we see that it has a few properties that lend itself well to Bayesian modeling:\n",
    "\n",
    "1) Considering the traditional softmax output, the elements are constrained to sum to 1.\n",
    "\n",
    "2) The elements correspond to probabilities so are nonnegative\n",
    "\n",
    "These conditions suggest a Dirichlet prior on the label space could be appropriate. First, the support of the Dirichlet distribution is nonnegative. Second, for the K-dimensional Dirichlet distribution, the support is a K-1 simplex, so it automatically has the property that the elements sum to 1.\n",
    "\n",
    "To incorporate this into the NCP KL divergence term, to encourage higher uncertainty for OOD data, we could use the KL divergence to a Dirichlet distribution that has the mean vector as the equiprobable point (all dimensions equally uncertain). For in-distribution data, the Dirichlet would be parameterized such that the mode is at the corner of the simplex corresponding to the given class label.\n",
    "\n",
    "One caveat is that the concentration parameters of the Dirichlet distribution must be positive but are allowed to be greater than 1. So we cannot keep our softmax layer, which imposes to strong a constraint. Instead we can use for example a softplus layer, i.e. log(1 + exp(x)), which is nonnegative and can be arbitrarily large. The interpretation of the network layer changes a little bit, so now it represents the parameters of a distribution which when sampled, have the properties we require of any probabilistic classification decision.\n",
    "\n",
    "Finally, one interesting aspect of this Dirichlet label prior is how the concentration paramters (alpha vector) simultaneously control both the mean and covariances of the distribution over classification probabilities. Compare this to our current work using an entropy uncertainty penalty which is invariant to permutations, i.e. the entropy is the same as long as the set has the same elements, but the order / structure does not matter. For this MNIST classification task, that may be a bad property: imagine the probability vector of an MNIST classifier which assigns equal classification probability to \"0\" and \"8\" (which is understandable) vs. another classifier which assigns equally high probabilities to \"0\" and \"4\" (which is less excusable). The current entropy formulation would give the same loss in both cases despite one being more tolerable than the other.\n",
    "\n",
    "The network would have K output neurons, each representing a concentration parameter of the Dirichlet. The network could look something like this:\n",
    "\n",
    "<img src=\"./images/NCP_BNN_dirichlet_categorical_classifier.PNG\" width=\"800\" />\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "-Another open question is what happens if we include output categories that are basically anomaly detectors? I.e. another neuron which is a catch-all category for things unfamiliar? But perhaps the best solution would be a dynamic network which can learn to increase/decrease the number of classification categories on the fly. In other words, a dynamic graph that can prune or grow nodes and have an architecture that changes during learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPENDIX: Additional Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: “rotate”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described earlier, a priori we might expect that doing such a simple transformation as rotation alone would not sufficiently move points away from in-distribution to any OOD regions that are worth sampling, i.e. by rotating a “7” by any arbitrary amount we will never recover an “8.” We seek to confirm or refute this with a rotation experiment.\n",
    "For a given set of holdout digit classes (e.g. {8,9}), and a reasonable alpha value, we vary the range of angles that the digits are allowed to rotate through to transform them from in-distribution to a synthetic sample of OOD data. We retrain the network from the same random seed 10 times. Each time, the range of allowed angles is increased, ranging from 0 degrees (no rotation at all so synthetic OOD looks exactly like in-distribution) to 180 degees. The actual angle of rotation for each image is chosen uniformly at random  from [–Theta,Theta].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sandervandenhaute/Documents/MLPROB/ProbabilisticProgramming6998/ncp_classifier/models/mnist_det.py:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost=6.094564092\n",
      "Epoch: 0002 cost=1.237203221\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.8814315\n",
      "Full od_acc: 0.73116183\n",
      "Epoch: 0001 cost=6.047517842\n",
      "Epoch: 0002 cost=1.237732139\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.8819502\n",
      "Full od_acc: 0.7116805\n",
      "Epoch: 0001 cost=5.939010039\n",
      "Epoch: 0002 cost=1.202573476\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.8897925\n",
      "Full od_acc: 0.6310996\n",
      "Epoch: 0001 cost=5.840448677\n",
      "Epoch: 0002 cost=1.175111916\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.8985477\n",
      "Full od_acc: 0.5657469\n",
      "Epoch: 0001 cost=5.741275329\n",
      "Epoch: 0002 cost=1.152782226\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.90678424\n",
      "Full od_acc: 0.511473\n",
      "Epoch: 0001 cost=5.667923265\n",
      "Epoch: 0002 cost=1.134241587\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9085892\n",
      "Full od_acc: 0.46201244\n",
      "Epoch: 0001 cost=5.612907799\n",
      "Epoch: 0002 cost=1.117858535\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.90935683\n",
      "Full od_acc: 0.41628632\n",
      "Epoch: 0001 cost=5.566091222\n",
      "Epoch: 0002 cost=1.103436448\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.91236514\n",
      "Full od_acc: 0.39029047\n",
      "Epoch: 0001 cost=5.520431876\n",
      "Epoch: 0002 cost=1.091147249\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.9139834\n",
      "Full od_acc: 0.36506224\n",
      "Epoch: 0001 cost=5.482566314\n",
      "Epoch: 0002 cost=1.087876555\n",
      "Optimization Finished!\n",
      "Full id_acc: 0.91184646\n",
      "Full od_acc: 0.33661824\n"
     ]
    }
   ],
   "source": [
    "# code running deterministic network, on the rotate experiment\n",
    "# parameters and mroe details are in the file itself:\n",
    "# see ncp_classifier/models/mnist_det.py\n",
    "\n",
    "#Reset tf graph in case you have already run one of our other experiments in this notebook session:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from ncp_classifier.models.mnist_det import rotation_experiment\n",
    "\n",
    "rotation_experiment()\n",
    "\n",
    "## During training, log files will be periodically saved in ncp_classifier/logs\n",
    "#Full id_acc:     accuracy over the entire partition of in-distribution data\n",
    "#Full od_acc:     accuracy over the entire partition of out-of-distribution data\n",
    "    \n",
    "#(Ignore Futurewarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running plotting script with experiment type\n",
      "rotate\n",
      "['ncp_classifier\\\\logs\\\\log_ncp_on__rotate_0.p', 'ncp_classifier\\\\logs\\\\log_ncp_on__rotate_1.p', 'ncp_classifier\\\\logs\\\\log_ncp_on__rotate_2.p', 'ncp_classifier\\\\logs\\\\log_ncp_on__rotate_3.p', 'ncp_classifier\\\\logs\\\\log_ncp_on__rotate_4.p']\n"
     ]
    }
   ],
   "source": [
    "# The above training saves out data in ncp_classifier/logs\n",
    "# Once above training is done, we can plot the results:\n",
    "\n",
    "from ncp_classifier.scripts.plotting import run_plotting\n",
    "\n",
    "# Run the plotting script\n",
    "run_plotting('rotate')\n",
    "\n",
    "# Output is saved in ncp_classifier/output/rotate\n",
    "\n",
    "# Example plots are shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/det_rotate_CE_loss.png\" width=\"500\" />\n",
    "\n",
    "<img src=\"./images/det_rotate_entropy.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: “successive holdout of digit classes”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we look at the effects of holding out (“omitting”) more and more digit classes, i.e. we try various bipartitions of the set {0,…,9} that leave successively fewer digit categories in the in-distribution training set. We constrain the problem to have at least two classes for in-distribution training, and at least one digit class omitted. In terms of training accuracy, we might expect the easiest classification problem to be when there are two classes, i.e. binary classification (instead of multiclass classification with some digits being visually similar). In this case we might expect uncertainty during training to also be low, and perhaps to lead to overconfident decisions when the model sees omitted (unseen) classes. Also, we might reason that the highest uncertainty on the omitted classes would occur when there are very many different omitted categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code running deterministic network, on the rotate experiment\n",
    "## parameters and mroe details are in the file itself:\n",
    "## see ncp_classifier/models/mnist_det.py\n",
    "\n",
    "\n",
    "from ncp_classifier.models.mnist_det import digits_out_experiment\n",
    "\n",
    "digits_out_experiment()\n",
    "\n",
    "## During training, log files will be periodically saved in ncp_classifier/logs\n",
    "#Full id_acc:     accuracy over the entire partition of in-distribution data\n",
    "#Full od_acc:     accuracy over the entire partition of out-of-distribution data\n",
    "    \n",
    "#(Ignore Futurewarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the plotting script\n",
    "run_plotting('digout')\n",
    "\n",
    "#Output is saved in ncp_classifier/output/alpha\n",
    "\n",
    "#Example plots are shown below, for a case with\n",
    "#a small in-distribution ser {0,1}\n",
    "#and large omitted set {2,3,4,5,6,7,8,9}:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/det_digout_accuracy.png\" width=\"500\" />\n",
    "\n",
    "<img src=\"./images/det_digout_entropy.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
