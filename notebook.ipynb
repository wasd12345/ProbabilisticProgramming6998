{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in neural networks using noise contrastive priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Based on **arXiv:1807.09289**)\n",
    "\n",
    "Neural networks are often very successful at making predictions for inputs that are in some sense similar to the training data. However, if the training data is not sufficiently diverse, then at test time, one will often encounter inputs that are *out-of-distribution (OOD)* and for which the network might yield unpredictable and inaccurate results -- as opposed to the *in-distribution (ID)* training data. In those cases, it would therefore be useful to have reliable estimates on the uncertainty of the prediction.\n",
    "\n",
    "Bayesian neural networks are a standard way of tackling this problem. During training, instead of learning point estimates for the weights and biases of the network, one learns a probability distribution over those parameters. At test time, one first samples the network parameters from the learned distributions before making a prediction. As such, a Bayesian neural network represents a distribution of functions, which for a given input yields a certain distribution of outputs. \n",
    "However, it is not clear exactly how to specify the prior distribution on the weights, or how such a network generalizes on OOD data seems rather arbitrary.\n",
    "\n",
    "A simple toy example is given in the following figure. A neural network is used to predict the mean and standard deviation of a scalar variable (it has a two-dimensional output layer).\n",
    "On the left, a simple deterministic network is used. On the right, a bayesian layer is introduced just before the output layer.\n",
    "\n",
    "<img src=\"./images/nn.png\" width=\"600\" />\n",
    "\n",
    "Even though the Bayesian approach does introduce uncertainty in the predicted mean (which depends on the posterior of the weights in the final hidden layer), the generalization to unseen data points is, in some sense, random.\n",
    "\n",
    "A recently proposed approach starts from the premise that, in order to encourage the network to output high uncertainty, it is enough to encourage this at the boundary of the training data. The procedure is as follows\n",
    "1. Perturb the input data to approximate OOD behavior (e.g. add noise) \n",
    "2. Stimulate the network to output a high uncertainty on the OOD data, by adding an additional contribution to the loss function.\n",
    "\n",
    "\n",
    "In the example of the Bayesian neural network, the proposed new loss function then looks like this\n",
    "\n",
    "$$\\large \\mathcal{L}_{\\text{NCP}} (\\phi) = \\mathcal{L}_{\\text{BBB}}(\\phi) {\\Big\\rvert}_{\\text{ID}} \\quad + \\quad \\lambda \\text{KL}\\left[ \\text{Normal}(\\mu_{\\mu}, \\sigma_{\\mu}^2) || q(\\mu(x)) \\right]\\Big\\rvert_{\\text{OOD}}$$\n",
    "\n",
    "in which the variance of the normal distribution $\\sigma^2$ is chosen very large, to stimulate uncertainty in the distribution of the mean when the network is fed with OOD inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project attempts to apply similar ideas in a classification setting. We want to see whether it is possible to train a neural network to output a higher uncertainty on unseen data and prevent overconfident classification, by adding an additional contribution to the loss function, similar to the previously discussed paper. The following graph explains the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram.svg\" width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification setting, we cannot just use the output distribution as a measure for uncertainty. **However, we can use the entropy of the probabilities from the softmax output layer to represent the uncertainty of the classifier.** This is an easy-to-calculate quantity. \n",
    "\n",
    "As for the generation of OOD data, multiple possibilities exist. Here, we chose to **apply affine transformations to the images.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for MNIST (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Deterministic network -- **no Bayesian layers!**\n",
    "\n",
    "2. Input data are 28x28 images of the digits 1, 2, 3, 4, 5, 6, 7 -- **8 and 9 are omitted, and are used for evaluation.**\n",
    "\n",
    "3. 256 --> 256 --> 8 network, using leaky ReLU activation functions and a softmax output.\n",
    "\n",
    "4. Training through Adam.\n",
    "\n",
    "5. **OOD data generated through rotations**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for defining network template, calculation of entropy and CE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code showing loss is difference between CE and entropy, scaled by alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for deterministic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(show images of decrease in accuracy, increase in entropy of both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "1. Accuracy decreases as NCP term in loss is weighted more strongly\n",
    "2. Entropy difference between ID and OOD data increases, but standard deviation relatively high --> **network still unable to tell the difference between input it has already seen vs new input**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for MNIST (2): addition of bayesian layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
